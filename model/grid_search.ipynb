{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "import numpy as np\n",
    "import torch \n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from math import ceil\n",
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mrnn import MilliesDataset, MilliesRNN\n",
    "from hessianfree import HessianFree\n",
    "\n",
    "\n",
    "\n",
    "# Load data\n",
    "whole_dataset = MilliesDataset('monkey_data.mat')\n",
    "dataset_size = len(whole_dataset)\n",
    "train_dataset, test_dataset = random_split(whole_dataset, [401, 101])\n",
    "\n",
    "in_dim, out_dim, trial_len = whole_dataset.dimensions() #  21  &  50\n",
    "hid_dim = 100\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=ceil(len(train_dataset)/5), shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=True)\n",
    "\n",
    "# for generating output later\n",
    "whole_dataloader = DataLoader(whole_dataset, batch_size = len(whole_dataset), shuffle=False) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.utils.prune as prune\n",
    "import torch.nn.functional as F\n",
    "import pickle \n",
    "\n",
    "from hessianfree import HessianFree\n",
    "\n",
    "hessian = False\n",
    "hardcore = True\n",
    "intermodule_connections_removed = .9\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "batch_size = [3,4,5]\n",
    "weight_decay = [0,1e-6,1e-5,1e-4]\n",
    "\n",
    "num_epochs = 20\n",
    "test_losses = {}\n",
    "\n",
    "print(\"When i = 0, clipping, when i = 1, no clipping\\n\")\n",
    "\n",
    "criterion1 = nn.MSELoss() \n",
    "\n",
    "for l in learning_rates:\n",
    "    for b in batch_size:\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=ceil(len(train_dataset)/b), shuffle=True)\n",
    "        for w in weight_decay:\n",
    "            for i in range(2):\n",
    "                print(\"Learning rate = \" + str(l) + \", batch size = \" + str(1/b) + \", i = \" + str(i) + \", weight decay = \" + str(w) + \"\\n\")\n",
    "                model_type = f\"mse_{l}_{b}_{i}_{w}\"\n",
    "                model = MilliesRNN(in_dim, hid_dim, out_dim, True)\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=l, weight_decay = w)\n",
    "                module1 = model.h2o\n",
    "                prune.random_unstructured(module1, name=\"weight\", amount=intermodule_connections_removed)\n",
    "                module2 = model.thal\n",
    "                prune.random_unstructured(module2, name=\"weight\", amount=intermodule_connections_removed)\n",
    "                model.train()\n",
    "                for epoch in range(num_epochs):\n",
    "                    for i, (inp_batch, out_batch) in enumerate(train_dataloader):\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        outputs = model(inp_batch)   \n",
    "\n",
    "                        loss = criterion1(outputs, out_batch)\n",
    "                        \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        \n",
    "                        if(i == 0):\n",
    "                            nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "                \n",
    "\n",
    "                num_samples = len(test_dataset)\n",
    "            \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    inp, out = next(iter(test_dataset))\n",
    "                    gen_out = model(inp)\n",
    "                    loss1 = criterion1(gen_out, out)\n",
    "\n",
    "                print(f\"Average test: {loss1 / num_samples}\\n\")\n",
    "                test_losses[model_type] = loss1 / num_samples\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    inp, out_true = next(iter(whole_dataloader))\n",
    "                    whole_out = model(inp)\n",
    "\n",
    "                with open(f'outputs/{model_type}.pickle', 'wb') as handle:\n",
    "                    pickle.dump(whole_out, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'outputs/test_loss_mse.pickle', 'wb') as handle:\n",
    "    pickle.dump(test_losses, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(test_losses.values()))\n",
    "print(min(test_losses.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
